//--------------------------------------------------------------------------
// Code generated by the SmartSoft MDSD Toolchain
// The SmartSoft Toolchain has been developed by:
//
// Service Robotics Research Center
// University of Applied Sciences Ulm
// Prittwitzstr. 10
// 89075 Ulm (Germany)
//
// Information about the SmartSoft MDSD Toolchain is available at:
// www.servicerobotik-ulm.de
//
// This file is generated once. Modify this file to your needs.
// If you want the toolchain to re-generate this file, please
// delete it before running the code generator.
//--------------------------------------------------------------------------
#include "CameraTask.hh"
#include "ComponentWebotsRobot.hh"
#include "webots/Node.hpp"

#include <iostream>

CameraTask::CameraTask(SmartACE::SmartComponent *comp)
    : CameraTaskCore(comp),
      image_counter(0),
      _camera(NULL)
{
  std::cout << "constructor CameraTask\n";
}
CameraTask::~CameraTask()
{
  delete _camera;
  std::cout << "destructor CameraTask\n";
}

int CameraTask::on_entry()
{
  if (!COMP->_supervisor ||
      !COMP->getParameters().getCamera_properties().getEnable())
    return -1;

  COMP->mRobotMutex.acquire();

  for (int i = 0; i < COMP->_supervisor->getNumberOfDevices(); i++)
  {
    auto webotsDevice = COMP->_supervisor->getDeviceByIndex(i);

    if (webotsDevice->getNodeType() == webots::Node::CAMERA)
    {
      _camera = dynamic_cast<webots::Camera *>(webotsDevice);
      _camera->enable(computeCameraUpdate());
      if (COMP->getParameters().getCamera_properties().getEnable_recognition())
        _camera->recognitionEnable(computeCameraUpdate());
      break;
    }
  }
  if (_camera == NULL)
  {
    std::cerr << "You enable a Camera, but robot has not such device!"
              << std::endl;
    COMP->mRobotMutex.release();
    return -1;
  }

  getCameraPoseRobotFrame();

  COMP->mRobotMutex.release();

  return 0;
}
int CameraTask::on_execute()
{
  if (!_camera || !COMP->_supervisor)
    return -1;

  COMP->mRobotMutex.acquire();

  auto image = _camera->getImage();
  if (image)
  {
    unsigned int cols = _camera->getWidth();
    unsigned int rows = _camera->getHeight();
    ++image_counter;
    comm_video_image.setSeq_count(image_counter);
    comm_video_image.set_parameters(cols, rows,
                                    DomainVision::FormatType::RGB32);
    comm_video_image.set_data(image);
    comm_video_image.setIs_valid(true);
    rgbd_image.setColor_image(comm_video_image);
    rgbd_image.setSeq_count(image_counter);
    rgbd_image.setIs_valid(true);

    if (COMP->getParameters().getCamera_properties().getEnable_recognition())
      recognition();
  }

  else
    rgbd_image.setIs_valid(false);

  // send out image through port
  COMP->rGBImagePushServiceOut->put(rgbd_image.getColor_image());

  COMP->mRobotMutex.release();
  return 0;
}
int CameraTask::on_exit()
{
  // use this method to clean-up resources which are initialized in on_entry() and needs to be freed before the on_execute() can be called again
  return 0;
}

int CameraTask::computeCameraUpdate() const
{
  double simulation_step = COMP->_supervisor->getBasicTimeStep();
  double camera_step = 1000.0 /
                       COMP->connections.cameraTask.periodicActFreq;
  int duration = (int)(ceil(camera_step / simulation_step) * simulation_step);
  std::cout << "Camera duration is: " << duration << "\n";
  return duration;
}

void CameraTask::recognition()
{
  auto robot_pose = webotsNodeToAffine3dGlobal(COMP->_supervisor->getSelf());
  CommObjectRecognitionObjects::CommObjectRecognitionEnvironment env;
  std::vector<CommObjectRecognitionObjects::
    CommObjectRecognitionObjectProperties> objs;

  int number_of_objects = _camera->getRecognitionNumberOfObjects();
  auto objects = _camera->getRecognitionObjects();
  for (int i = 0; i < number_of_objects; ++i)
  {
    CommObjectRecognitionObjects::CommObjectRecognitionObjectProperties
        obj_properties;
    obj_properties.setObject_type(objects[i].model);
    obj_properties.setObject_id(objects[i].id);

    Eigen::Affine3d object_frame = (Eigen::Affine3d)Eigen::AngleAxisd(
        objects[i].orientation[3], Eigen::Vector3d(objects[i].orientation[0], 
        objects[i].orientation[1], objects[i].orientation[2]));
    object_frame.translation() = Eigen::Vector3d(
        objects[i].position[0] * 1000, objects[i].position[1] * 1000,
        objects[i].position[2] * 1000);

    object_frame = robot_pose * _camera_pose * object_frame;

    auto pose = affine3dToPose3d(object_frame);
    nedToEnu(pose);
    obj_properties.setPose(pose);
    obj_properties.set_dimension(objects[i].size[0], objects[i].size[1], 0);
    obj_properties.setIs_valid(true);

    std::vector<CommObjectRecognitionObjects::CommObjectDominantColor>
        colors;
    for (size_t j = 0; j < objects[i].number_of_colors; ++j)
    {
      CommObjectRecognitionObjects::CommObjectDominantColor color;
      color.setR(objects[i].colors[3 * j]);
      color.setG(objects[i].colors[3 * j + 1]);
      color.setB(objects[i].colors[3 * j + 2]);
      color.setDominance(1.0 / objects[i].number_of_colors);
      colors.push_back(color);
    }
    obj_properties.setObject_colors(colors);
    objs.push_back(obj_properties);

    /*
		printf("Relative position of object %d: %lf %lf %lf\n", i, 
			objects[i].position[0], objects[i].position[1], 
			objects[i].position[2]);
		printf("Relative orientation of object %d: %lf %lf %lf %lf\n", i, 
			objects[i].orientation[0], objects[i].orientation[1],
			objects[i].orientation[2], objects[i].orientation[3]);
		printf("Size of object %d: %lf %lf\n", i, objects[i].size[0], 
			objects[i].size[1]);
		printf("Id of object %d: %d\n", i, objects[i].id);	
		printf("Model of object %d: %s\n", i, objects[i].model);
		printf("Position of the object %d on the camera image: %d %d\n", i, 
			objects[i].position_on_image[0], objects[i].position_on_image[1]);
		printf("Size of the object %d on the camera image: %d %d\n", i, 
			objects[i].size_on_image[0], objects[i].size_on_image[1]);
		for (int j = 0; j < objects[i].number_of_colors; ++j)
			printf("- Color %d/%d: %lf %lf %lf\n", j + 1, 
				objects[i].number_of_colors, objects[i].colors[3 * j],
				objects[i].colors[3 * j + 1], objects[i].colors[3 * j + 2]);
		*/
  }
  env.setObjects(objs);
  env.setIs_valid(true);
  objectsPushServiceOutPut(env);
}

void CameraTask::getCameraPoseRobotFrame()
{
  _camera_pose = Eigen::Affine3d::Identity();
  if (!_camera || !COMP->has_supervisor)
    return;

  auto matrix_indexes = COMP->getParameters().getCamera_properties().getCamera_rotation_matrix();
  auto translation_indexes = COMP->getParameters().getCamera_properties().getCamera_translation();
  std::vector<double> m{std::make_move_iterator(std::begin(matrix_indexes)),
                        std::make_move_iterator(std::end(matrix_indexes))};
  std::vector<double> t{
      std::make_move_iterator(std::begin(translation_indexes)),
      std::make_move_iterator(std::end(translation_indexes))};

  Eigen::Matrix3d R;
  if (matrix_indexes.size() == 9 && t.size() == 3)
  {
    R << m[0], m[1], m[2],
        m[3], m[4], m[5],
        m[6], m[7], m[8];
    _camera_pose.linear() = R;
    _camera_pose.translation() = Eigen::Vector3d(t[0] * 1000, t[1] * 1000,
                                                 t[2] * 1000);
    return;
  }

  // This may not be the actual pose of the camera wrt the robot
  // frame, since it is possble to define a rotation in a nested
  // proto. The best way is to provide the matrix as a parameter
  webots::Node *robot = COMP->_supervisor->getSelf();
  auto children = robot->getField("cameraSlot");
  if (!children)
    return;

  size_t number_of_nodes = children->getCount();
  for (size_t i = 0; i < number_of_nodes; ++i)
  {
    auto node = children->getMFNode(i);
    if (node->getType() == webots::Node::CAMERA)
    {
      _camera_pose = webotsNodeToAffine3d(node);
      return;
    }
  }
}